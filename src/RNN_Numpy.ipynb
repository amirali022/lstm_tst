{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [\n",
    "\t\"good\",\n",
    "\t\"bad\",\n",
    "\t\"happy\",\n",
    "\t\"sad\",\n",
    "\t\"not good\",\n",
    "\t\"not bad\",\n",
    "\t\"not happy\",\n",
    "\t\"not sad\",\n",
    "\t\"very good\",\n",
    "\t\"very bad\",\n",
    "\t\"very happy\",\n",
    "\t\"very sad\",\n",
    "\t\"i am happy\",\n",
    "\t\"this is good\",\n",
    "\t\"i am bad\",\n",
    "\t\"this is bad\",\n",
    "\t\"i am sad\",\n",
    "\t\"this is sad\",\n",
    "\t\"i am not happy\",\n",
    "\t\"this is not good\",\n",
    "\t\"i am not bad\",\n",
    "\t\"this is not sad\",\n",
    "\t\"i am very happy\",\n",
    "\t\"this is very good\",\n",
    "\t\"i am very bad\",\n",
    "\t\"this is very sad\",\n",
    "\t\"this is very happy\",\n",
    "\t\"i am good not bad\",\n",
    "\t\"this is good not bad\",\n",
    "\t\"i am bad not good\",\n",
    "\t\"i am good and happy\",\n",
    "\t\"this is not good and not happy\",\n",
    "\t\"i am not at all good\",\n",
    "\t\"i am not at all bad\",\n",
    "\t\"i am not at all happy\",\n",
    "\t\"this is not at all sad\",\n",
    "\t\"this is not at all happy\",\n",
    "\t\"i am good right now\",\n",
    "\t\"i am bad right now\",\n",
    "\t\"this is bad right now\",\n",
    "\t\"i am sad right now\",\n",
    "\t\"i was good earlier\",\n",
    "\t\"i was happy earlier\",\n",
    "\t\"i was bad earlier\",\n",
    "\t\"i was sad earlier\",\n",
    "\t\"i am very bad right now\",\n",
    "\t\"this is very good right now\",\n",
    "\t\"this is very sad right now\",\n",
    "\t\"this was bad earlier\",\n",
    "\t\"this was very good earlier\",\n",
    "\t\"this was very bad earlier\",\n",
    "\t\"this was very happy earlier\",\n",
    "\t\"this was very sad earlier\",\n",
    "\t\"i was good and not bad earlier\",\n",
    "\t\"i was not good and not happy earlier\",\n",
    "\t\"i am not at all bad or sad right now\",\n",
    "\t\"i am not at all good or happy right now\",\n",
    "\t\"this was not happy and not good earlier\"\n",
    "]\n",
    "\n",
    "train_y = [\n",
    "\t1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = [\n",
    "\t\"this is happy\",\n",
    "\t\"i am good\",\n",
    "\t\"this is not happy\",\n",
    "\t\"i am not good\",\n",
    "\t\"this is not bad\",\n",
    "\t\"i am not sad\",\n",
    "\t\"i am very good\",\n",
    "\t\"this is very bad\",\n",
    "\t\"i am very sad\",\n",
    "\t\"this is bad not good\",\n",
    "\t\"this is good and happy\",\n",
    "\t\"i am not good and not happy\",\n",
    "\t\"i am not at all sad\",\n",
    "\t\"this is not at all good\",\n",
    "\t\"this is not at all bad\",\n",
    "\t\"this is good right now\",\n",
    "\t\"this is sad right now\",\n",
    "\t\"this is very bad right now\",\n",
    "\t\"this was good earlier\",\n",
    "\t\"i was not happy and not good earlier\"\n",
    "]\n",
    "\n",
    "test_y = [1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set( [ q for text in train_X for q in text.split()])\n",
    "vocab_size = len( vocab)\n",
    "\n",
    "word_to_index = { w: i for i, w in enumerate( vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happy': 0,\n",
       " 'and': 1,\n",
       " 'is': 2,\n",
       " 'right': 3,\n",
       " 'or': 4,\n",
       " 'sad': 5,\n",
       " 'i': 6,\n",
       " 'bad': 7,\n",
       " 'good': 8,\n",
       " 'at': 9,\n",
       " 'all': 10,\n",
       " 'not': 11,\n",
       " 'earlier': 12,\n",
       " 'now': 13,\n",
       " 'was': 14,\n",
       " 'am': 15,\n",
       " 'very': 16,\n",
       " 'this': 17}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def oneHotEncode( text):\n",
    "\tinputs = []\n",
    "\n",
    "\tfor q in text.split():\n",
    "\t\tvector = np.zeros( ( 1, vocab_size))\n",
    "\t\tvector[ 0][ word_to_index[ q]] = 1\n",
    "\t\tinputs += [ vector]\n",
    "\n",
    "\treturn inputs\n",
    "\n",
    "# Xavier Normalized Initialization\n",
    "def initWeight( input_size, output_size):\n",
    "\treturn np.random.uniform( -1, 1, ( input_size, output_size)) * np.sqrt( 6 / ( input_size + output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh( input, derivative = False):\n",
    "\tif derivative:\n",
    "\t\t# return 1 - ( tanh( input) ** 2)\n",
    "\t\t# because we will store the value of tanh(x) in network, instead of just x\n",
    "\t\treturn 1 - ( input ** 2)\n",
    "\t\n",
    "\treturn np.tanh( input)\n",
    "\n",
    "def softmax( input):\n",
    "\treturn np.exp( input) / np.sum( np.exp( input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recurrent Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\tdef __init__( self, input_size, hidden_size, output_size):\n",
    "\t\t# Network weight\n",
    "  \n",
    "\t\t# input weights\n",
    "\t\tself.w1 = initWeight( input_size, hidden_size)\n",
    "\t\t# hidden state weights\n",
    "\t\tself.w2 = initWeight( hidden_size, hidden_size)\n",
    "\t\t# output weights\n",
    "\t\tself.w3 = initWeight( hidden_size, output_size)\n",
    "\n",
    "\t\t# biases\n",
    "\n",
    "\t\t# there is no bias for first layer ( b2 would suffice for hidden state function)\n",
    "\n",
    "\t\t# hidden state bias\n",
    "\t\tself.b2 = np.zeros( ( 1, hidden_size))\n",
    "\t\t# output bias\n",
    "\t\tself.b3 = np.zeros( ( 1, output_size))\n",
    "\n",
    "\t# Forward Propagation\n",
    "\tdef forward( self, inputs):\n",
    "\n",
    "\t\tself.hidden_states = [ np.zeros_like( self.b2)]\n",
    "\n",
    "\t\tfor input in inputs:\n",
    "\t\t\tlayer1_output = np.dot( input, self.w1)\n",
    "\t\t\tlayer2_output = np.dot( self.hidden_states[ -1], self.w2) + self.b2\n",
    "\n",
    "\t\t\tself.hidden_states += [ tanh( layer1_output + layer2_output)]\n",
    "\n",
    "\t\treturn np.dot( self.hidden_states[ -1], self.w3) + self.b3\n",
    "\t\n",
    "\t# Backward Propagation\n",
    "\tdef backward( self, error, input, learning_rate):\n",
    "\t\td_b3 = error\n",
    "\t\td_w3 = np.dot( self.hidden_states[ -1].T, error)\n",
    "\n",
    "\t\td_b2 = np.zeros_like( self.b2)\n",
    "\t\td_w2 = np.zeros_like( self.w2)\n",
    "\t\td_w1 = np.zeros_like( self.w1)\n",
    "\n",
    "\t\td_hidden_state = np.dot( error, self.w3.T)\n",
    "\n",
    "\t\tfor q in reversed( range( len( input))):\n",
    "\t\t\td_hidden_state *= tanh( self.hidden_states[ q + 1], derivative=True)\n",
    "\n",
    "\t\t\td_b2 += d_hidden_state\n",
    "\n",
    "\t\t\td_w2 += np.dot( self.hidden_states[ q].T, d_hidden_state)\n",
    "\n",
    "\t\t\td_w1 += np.dot( input[ q].T, d_hidden_state)\n",
    "\n",
    "\t\t\td_hidden_state = np.dot( d_hidden_state, self.w2)\n",
    "\n",
    "\t\tfor d_ in ( d_b3, d_w3, d_b2, d_w2, d_w1):\n",
    "\t\t\tnp.clip( d_, -1, 1, out=d_)\n",
    "\n",
    "\t\tself.b3 += learning_rate * d_b3\n",
    "\t\tself.w3 += learning_rate * d_w3\n",
    "\t\tself.b2 += learning_rate * d_b2\n",
    "\t\tself.w2 += learning_rate * d_w2\n",
    "\t\tself.w1 += learning_rate * d_w1\n",
    "\n",
    "\t# Training Method\n",
    "\tdef fit( self, inputs, labels, epochs, learning_rate):\n",
    "\t\tfor _ in tqdm( range( epochs)):\n",
    "\t\t\tfor input, label in zip( inputs, labels):\n",
    "\t\t\t\tinput = oneHotEncode( input)\n",
    "\n",
    "\t\t\t\tprediction = self.forward( input)\n",
    "\n",
    "\t\t\t\terror = -softmax( prediction)\n",
    "\n",
    "\t\t\t\terror[ 0][ label] += 1\n",
    "\n",
    "\t\t\t\tself.backward( error, input, learning_rate)\n",
    "\n",
    "\tdef predict( self, inputs, labels):\n",
    "\t\thit = 0\n",
    "\n",
    "\t\tfor input, label in zip( inputs, labels):\n",
    "\t\t\tprint( input)\n",
    "\n",
    "\t\t\tinput = oneHotEncode( input)\n",
    "\t\t\tprediction = self.forward( input)\n",
    "\n",
    "\t\t\tprint( [ \"Negative\", \"Positive\"][ np.argmax( prediction)], end=\"\\n\\n\")\n",
    "\n",
    "\t\t\tif np.argmax( prediction) == label:\n",
    "\t\t\t\thit += 1\n",
    "\n",
    "\t\tprint( f\"Accuracy: { hit / len( inputs) * 100}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 71.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is happy\n",
      "Positive\n",
      "\n",
      "i am good\n",
      "Positive\n",
      "\n",
      "this is not happy\n",
      "Negative\n",
      "\n",
      "i am not good\n",
      "Negative\n",
      "\n",
      "this is not bad\n",
      "Positive\n",
      "\n",
      "i am not sad\n",
      "Positive\n",
      "\n",
      "i am very good\n",
      "Positive\n",
      "\n",
      "this is very bad\n",
      "Negative\n",
      "\n",
      "i am very sad\n",
      "Negative\n",
      "\n",
      "this is bad not good\n",
      "Negative\n",
      "\n",
      "this is good and happy\n",
      "Positive\n",
      "\n",
      "i am not good and not happy\n",
      "Negative\n",
      "\n",
      "i am not at all sad\n",
      "Positive\n",
      "\n",
      "this is not at all good\n",
      "Negative\n",
      "\n",
      "this is not at all bad\n",
      "Positive\n",
      "\n",
      "this is good right now\n",
      "Positive\n",
      "\n",
      "this is sad right now\n",
      "Negative\n",
      "\n",
      "this is very bad right now\n",
      "Negative\n",
      "\n",
      "this was good earlier\n",
      "Positive\n",
      "\n",
      "i was not happy and not good earlier\n",
      "Negative\n",
      "\n",
      "Accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(\n",
    "\tinput_size=vocab_size,\n",
    "\thidden_size=64,\n",
    "\toutput_size=2\n",
    ")\n",
    "\n",
    "rnn.fit(\n",
    "\tinputs=train_X,\n",
    "\tlabels=train_y,\n",
    "\tepochs=1000,\n",
    "\tlearning_rate=0.02\n",
    ")\n",
    "\n",
    "rnn.predict(\n",
    "\tinputs=test_X,\n",
    "\tlabels=test_y\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
